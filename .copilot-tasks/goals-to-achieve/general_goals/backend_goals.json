{
  "backend_goals": [
    {
      "id": "backend_core_scope",
      "title": "Crew-7 Backend — Core Responsibilities",
      "copilot_prompt": "Backend scope for Crew-7.\n\nThe backend must:\n1) Provide REST + SSE + WebSocket APIs for crews, runs, traces, tools, marketplace, LLM connections, environment variables, and workspace/org management.\n2) Execute multi-agent systems using CrewAI (or equivalent) with full run lifecycle management: queued, running, paused, resumed, succeeded, failed, cancelled.\n3) Produce granular run events (agent_start, agent_end, tool_call, tool_result, graph_edge, user_message, system_message) and stream them in real time.\n4) Enforce strict sandboxing for tools (git, pytest, bash, file ops, GitHub PR) via docker or constrained subprocess execution.\n5) Store artifacts, run logs, and trace history with pagination, filters, and soft retention limits.\n6) Support per-org secret storage (LLM keys, environment variables) with encryption and masked read access.\n7) Be multi-tenant safe: no cross-org leakage, per-org limits, and clear auth checks on every request.\n8) Keep APIs predictable and stable; prefer additive changes over breaking ones.\n\nThe backend must NOT:\n- Become a monolithic code generator.\n- Store raw secrets in DB, logs, or traces.\n- Execute unbounded or unsafe shell commands.\n- Handle frontend presentation concerns.\n\nCopilot — procedure for any backend change:\n1) First, scan the codebase for existing modules related to the change (services, routers/controllers, models, tests).\n2) Confirm that new code fits these rules and this architecture.\n3) Extend existing behavior instead of rewriting unless there is a clear bug.\n4) If a requested change conflicts with these constraints, add a comment explaining the risk and propose a safer alternative."
    },
    {
      "id": "backend_runs_and_traces",
      "title": "Runs, Traces & Events",
      "copilot_prompt": "Runs + traces goals:\n1) Every crew execution must create a Run record and an associated TraceBatch (or equivalent root trace entity).\n2) Every agent action must produce trace events: agent_start, agent_output/agent_token (streaming text), agent_end (with status and summary).\n3) All tool executions must be captured as tool_call + tool_result with timing, status, and payload metadata.\n4) WebSocket endpoints (e.g. /ws/mission, /ws/graph, /ws/logs) must broadcast events in well-structured JSON with a stable type field (e.g. 'event_type').\n5) API must allow run control: pause, resume, cancel, and optionally retry; these state changes must also emit events.\n6) Storing run artifacts (files, logs, summaries) must be bounded in size and associated with org/workspace IDs.\n7) Traces must be queryable by run_id, crew_id, status, created_at, and org_id.\n\nCopilot:\n- When editing run/trace/websocket code, always check existing event types and states first.\n- Ensure no lifecycle step is skipped: queued -> running -> (paused/resumed)* -> finished (success/failure/cancelled).\n- Add tests or fixtures when introducing new event types or states."
    },
    {
      "id": "backend_mission_bus_and_ws",
      "title": "Mission Bus & Live Graph Streaming",
      "copilot_prompt": "Mission bus / WebSocket behavior:\n1) There must be a central event bus (Redis or in-memory) that receives mission events from workers and fans them out to WebSocket clients.\n2) WebSocket connections must authenticate using the same access token mechanism as HTTP APIs.\n3) The mission/graph stream must include: run status updates, agent node activity, tool edges, errors, alerts, and orchestrator messages.\n4) Payloads should be small, structured, and versioned (include 'v' or 'schema_version' when needed).\n5) Disconnections and reconnections should be handled gracefully; clients can resubscribe with run_id.\n\nCopilot:\n- Before modifying mission bus or WebSocket handlers, scan for existing pub/sub, event enums, and DTO/serializer types.\n- Ensure new events are documented and do not silently break current consumers.\n- Where possible, add comments describing the contract between backend and frontend."
    },
    {
      "id": "backend_marketplace",
      "title": "Marketplace and Crew Templates",
      "copilot_prompt": "Marketplace backend rules:\n1) Templates define crews (nodes, tools, prompts, connections) and can be installed into the user's workspace.\n2) Installation must be idempotent: running the same install twice must never duplicate or overwrite existing user-specific crews; use unique IDs / slugs per org.\n3) Installed templates become normal crews with their own IDs, settings, and graphs; they can evolve independently from the original template.\n4) Templates are stored as JSON + metadata (category, difficulty, tags, required providers, etc.).\n5) Marketplace listing endpoints must support pagination, filters (category, level, tags), and search.\n6) Marketplace item details must include the orchestrator + 6 specialists breakdown and computed portfolio stats (XP, missions, success rate, rating).\n\nCopilot:\n- When working on marketplace endpoints, keep a strict separation between template definitions and runtime crew state.\n- Never mutate template JSON based on specific runs or orgs.\n- Ensure marketplace APIs are read-only for templates and write-only for user installations."
    },
    {
      "id": "backend_env_and_llm",
      "title": "LLM Connections & Env Vars",
      "copilot_prompt": "Environment + LLM scope:\n1) Store API keys encrypted with a server-side key; only ever return masked values (e.g. 'sk-****1234').\n2) Support pluggable LLM providers: OpenAI-compatible, Ollama, Anthropic (future), etc. Use a provider abstraction instead of hardcoding a single vendor.\n3) LLM connections must be scoped per org/workspace and never shared across tenants.\n4) Environment variables must support key/value pairs at org, workspace, and crew level; merging rules must be deterministic.\n5) Inject env vars into runs and tools, but never write them into logs, traces, or artifacts.\n\nCopilot:\n- When touching LLM or env-var code, always check encryption, masking, and scoping.\n- Reject any attempt to log full API keys or secrets; add redaction helpers if missing.\n- Prefer configuration objects and provider interfaces over ad-hoc HTTP calls."
    },
    {
      "id": "backend_security_limits",
      "title": "Sandbox & Security Rules",
      "copilot_prompt": "Security/sandbox rules:\n1) All tool executions (bash, git, tests, HTTP, etc.) must run with CPU, RAM, and time limits; enforce via docker, firejail, or constrained subprocess.\n2) Tool processes must be isolated from the main host environment and never have direct access to system-level files or private network resources.\n3) Network access should be disabled by default and only allowed for whitelisted domains (e.g. GitHub) when explicitly needed.\n4) Validate and sanitize all inputs coming from the frontend (IDs, filenames, URLs, prompts, etc.).\n5) Apply authentication + authorization checks on every endpoint using org_id and user_id context.\n6) Avoid dynamic eval or executing arbitrary Python/JS without sandbox.\n\nCopilot:\n- When adding new tools or execution paths, ensure they go through the sandbox layer.\n- Warn in comments if a proposed change weakens isolation or introduces unsanitized user input."
    },
    {
      "id": "backend_xp_and_portfolios",
      "title": "XP, Levels & Crew Portfolios",
      "copilot_prompt": "XP and portfolio system:\n1) Each crew and each agent must have an XP counter and level derived from XP using a clear formula or table.\n2) XP is primarily granted by completed missions/runs; amount may depend on mission size or rating.\n3) Portfolio stats to maintain: missions_completed, success_rate, client_rating_avg, hours_worked_estimate, artifacts_count.\n4) Level, rarity_tier, and valuation should be computed from XP + performance metrics, not hardcoded.\n5) APIs must expose a read-only portfolio view for marketplace cards and dashboards.\n\nCopilot:\n- Scan the codebase for existing XP/level fields and normalize them into a single source of truth (service or module).\n- When adding XP updates, ensure they are idempotent and transactional with mission completion.\n- Avoid heavy recalculation on every request; consider precomputed aggregates or background jobs."
    },
    {
      "id": "backend_web3_integration",
      "title": "Web3 & NFT Metadata Integration",
      "copilot_prompt": "Web3 integration rules:\n1) Represent crews and agents as digital assets via a metadata layer compatible with standard NFT metadata (name, description, image, attributes, etc.) plus Crew-7 specific fields (XP, level, missions, rarity_tier).\n2) Keep all chain-specific logic behind a thin adapter/service; the core backend should depend on interfaces, not directly on web3 libraries.\n3) Provide helpers to map internal models (Crew, Agent, Portfolio) into metadata JSON objects.\n4) Do NOT require on-chain deployment for local/dev use; allow off-chain or mock storage of metadata.\n5) Never expose private keys or signing material from the backend.\n\nCopilot:\n- When implementing or editing Web3-related code, start from the metadata mapping and keep contracts optional.\n- Ensure no on-chain call is made in a tight request path without timeouts and error handling.\n- Document the metadata fields clearly so the frontend and contracts can rely on them."
    },
    {
      "id": "backend_analytics_and_observability",
      "title": "Logging, Metrics & Health",
      "copilot_prompt": "Observability goals:\n1) Structured logging (JSON or equivalent) with request_id / correlation_id on every request and run.\n2) Basic metrics: request counts, latencies, error rates, active runs, tool failures, WS connections.\n3) Health endpoints: /health or /status with checks for DB, cache, and event bus.\n4) Errors and exceptions should be logged with enough context but without leaking secrets or full prompts.\n\nCopilot:\n- When adding new services, ensure they emit logs and reuse the common logger.\n- Avoid console.log-style debugging; prefer structured logs.\n- Add comments when a function is critical to run reliability or billing."
    },
    {
      "id": "backend_testing_and_migrations",
      "title": "Tests, Fixtures & Schema Migrations",
      "copilot_prompt": "Quality + migrations:\n1) Every critical module (runs, traces, marketplace, env/LLM, XP) should have at least minimal unit or integration tests.\n2) DB schema changes must be managed via migrations, with safe up/down steps.\n3) Provide seed/fixture data for local dev: a few example crews, agents, and marketplace items.\n4) Do not break existing tests; if behavior intentionally changes, update tests with clear reasoning in comments.\n\nCopilot:\n- Before large refactors, run tests (or at least ensure test files still import correctly).\n- For new features, add or extend tests in the same PR / change set.\n- When touching models or migrations, verify that all references in services and APIs remain valid."
    }
  ]
}
